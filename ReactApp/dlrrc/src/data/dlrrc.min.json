{"totalScore": 100.0, "parts": [{"no": "I", "label": "Basic information and Data Acqusition", "totalScore": 10.0, "items": [{"no": 1, "label": "Labels are meaningful and biological discrepancy, mentioning potential topological differences existing.", "description": "Labels are expected for meaning, so as to match the needs of clinic & radiology. Considering the black box of training, labels should be differenet in pathology or physiology, for a better interpretability and rationality. For example: 1) different pathological subtypes in classification tasks; 2) different tissues in segmentation tasks, like tumors and normal tissue.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 2, "label": "Filtration of radiological data are applied and described.", "description": "The radiological data should be checked for quality and phases after data acquisition. Cases with poor quality or are noisy should be eliminated. Elimination of cases should be declared in reports. The adjustment of scales for adapting models and data balancing is encouraged to declare in articles.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 3, "label": "Radiological data types:\n-\tIf several modalities are involved, each type of modality should have an acceptable ratio with detailed descriptions.\n-\tIf only one modality is involved, declaration is required.", "description": "We require authors to provide clear application of modals. If several modals are used in a report, authors should declare specific application circumstances of each modal. Difference between modal ratios of cases can be potential cause of incorrect feature extraction.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 4, "label": "For data sources:\n-\tIf it is all originated from open sources, situation of application and filters should be declared.\n-\tIf it involves closed-source data, institutional ethical reviewing approvement and serial numbers are required.", "description": "Detailed application should be listed if raw data involves open source dataset, like TCIA. The detailed application should include but are not limited to inclusion/exclusion criteria and repertoires of applied cases. Institutional ethical reviewing and approval number are required if non-open data is used.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}]}, {"no": "II", "label": "Pre-processing", "totalScore": 27.5, "items": [{"no": 5, "label": "The ratio of each label should be unextreme and acceptable.", "description": "We suggest that authors should apply appropriate datasets for model establishment. The \"appropriate\" term is defined as close proportions of labels. Extreme and imbalance datasets will ruin model performance. For example: 1) in a dichotomy research, the ratio of type A and type B should be close if special imbalance-related strategy doesn't exist. If several subtypes are involved in type A, ratios of these subtypes should be close too. 2)in a multi-label research, ratio of each label should be close. Definition of \"close\" xis the ratio between any two types in datasets should be less than 200% and higher than 50%.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 6, "label": "The ratio of generated images should be closed, otherwise reasonable explanation is required.", "description": "We don't suggest methods that balance datasets by adjustment of generating images in imbalance datasets. If type A have 40 cases and type B have 20 cases, ratios of their generated images should also be close to 40:20 when case sizes are similar. However, some existed reports adjust imbalance datasets in this method: they generate less images from type A and more images from type B, causing close quantity in images. This method is controversial for possible feature losing of type A, which isn't encouraged.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 7, "label": "Processing staffs are radiological professionals.", "description": "Pre-processing is processed with radiological staffs, which can be regarded as standard operations. Authors should describe these details and list staffs in corresponding authors.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 8, "label": "Pre-processing is processed by the gold standard guidance.", "description": "We suggest pre-processing is processed with existing gold standard results, which can partly promise interpretability and consistency of ground truth and training datasets. Considering for operational feasibility, it is comprehensible that process with visible features or macroanatomy but unsupported in the future, that is why we don't score for this method.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 9, "label": "Effective methods exist to promise accuracy of pre-processing.", "description": "Considering for potential effects of manual pre-processing, we require researchers  to apply necessary methods to reduce unanticipated impacts, like cross-validation in pre-processing.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 10, "label": "The ratio of each dataset is reasonable.", "description": "We require reasonable ratios of training/validation/testing datasets. It is unacceptable if testing datasets and validation datasets are small than 10% of all. Performance assessment will be incomplete and risky with too small testing datasets. If small testing dataset must use, necessary explanation is required.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 11, "label": "Cases are independent, which aren't involved in different datasets.", "description": "We require authors do not establish datasets with selection of generated images. Instead, datasets should be established with selection of cases. If images from one case are involved in training and testing datasets, performance indexes will be inflating and robustness will be damaged.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 12, "label": "Datasets are established by random selection, without manual manipulating.", "description": "We don't suggest dispensable manual processing in DL-Radiomics, which will damage robustness and reproducibility possibly. Manual manipulating in dataset establishment is not encouraged, as a method that possible affect reproducibility.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 13, "label": "Examples of pre-processing are listed.", "description": "Pre-processing examples can be an evidence of high-quality data in reports. Also, imperfections or mistakes of these processes can be easily found in figures.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 14, "label": "Methods of data augmentation are suited and correctly applied.", "description": "Data augmentation is widely used in Deep Learning. However, not every methods are suited for Radiomics and Translational Medicine. We don't suggest applying methods that possibly cause pixel information changing and fake feature generation, like Gaussian Noise. Also, overaggressive strategy is not encouraged, as no meaningful improvement in performance.", "totalScore": 5.0, "options": [{"answer": "Suited and correctly applied", "score": 5.0}, {"answer": "Suited but incorrectly applied or not suited but correctly applied", "score": 2.5}, {"answer": "Not suited nor correctly applied", "score": 0.0}]}]}, {"no": "III", "label": "Model and Dependence", "totalScore": 10.0, "items": [{"no": 15, "label": "Methods to avoid overfitting are applied and described.", "description": "We encourage some new methods and strategy application to avoid over-fitting, like some methods of escaping saddle points. The reason of choosing these method are encouraged to explain in article. Appropriate end of training can be admitted as an effective method to avoid over-fitting.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 16, "label": "Software environment should be listed, like serial numbers of version.", "description": "We require authors to provide detailed information about dependent software, which isn't hard for a real research. Not only Python and CUDA/cuDNN versions, details of Tensorflow/PyTorch and package are required too. It is admitted if these information are listed in open-access code package and can be searched.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 17, "label": "Hardware details should be listed.", "description": "Hard-wares are important in DL model deployment. These information can be used in authenticity and reproducibility assessment with other items and manufacturer information.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 18, "label": "Applied models are suited for tasks.", "description": "The \"suited\" term is defined as: 1) applied models are suited for research tasks. For example, ResNet is suited for two-label classification tasks. 2) Applied models are suited for data size. For example, ResNet-18 is suited for small datasets, and ResNet-101 is not suited. 3) Applied models can be deployed in hardware environment. For example, 3D U-Net usually can't deploy in workstations with small graphic memory.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}]}, {"no": "IV", "label": "Training, Validation and Testing", "totalScore": 27.5, "items": [{"no": 19, "label": "Training details are listed, like epoch and time spent.", "description": "We encourage authors to provide training details, including epochs of training, spent time in each epoch and a total spent time in training. These details can be used in reproduction and authenticity assessments.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 20, "label": "Hyperparameters are listed (at least including batch size and learning rate).", "description": "We require authors to provide detailed hyper-parameters in articles, which are important in model training and non-technical sensitive. The baseline is that batch size and learning rate are provided in articles.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 21, "label": "The curves of accuracy-epoch and loss-epoch are provided.", "description": "These curves can be evaluation basis of model well-trained, and also play an important role in reproduction and authenticity assessment.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 22, "label": "The end of training is decided by the performance trends in validation datasets and is described.", "description": "As a \"black box\" process, model training situation can't measure directly and is usually speculated by trends of performance variation in validation datasets or internal testing datasets. We suggest authors to decide training end with accuracy/loss-epoch curves.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 23, "label": "Methods to promote robustness are applied.", "description": "Robustness promotion methods are encouraged to apply in research. For example, cross-validation in annotation and 10-fold cross-validation in training can be regarded as effective methods in robustness promotion.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 24, "label": "The details of initial weights are described.", "description": "Initial weights are required to be declared in articles, which is meaningful for reproducibility. For transfer learning, we suggest authors to provide initial weights if pre-training is applied in open source data like Cifar-100 and ImageNet.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 25, "label": "Training workloads are listed.", "description": "We suggest authors to provide workloads of training, which can be used to assess reproducibility with other items. Also, for some reports targeted in edge deployment, training workloads is a key index for feasibility evaluation.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 26, "label": "Objective indexes are reasonable and complete.", "description": "We require authors to provide suitable objective indexes about model performance. These indexes should be suitable for tasks. For example, F1 score is suitable in mutli-label classification tasks, but is not suitable in dichotomy classification tasks.", "totalScore": 5.0, "options": [{"answer": "Reasonable and complete", "score": 5.0}, {"answer": "Reasonable but incomplete or not reasonable but complete", "score": 2.5}, {"answer": "Not reasonable nor complete", "score": 0.0}]}, {"no": 27, "label": "Comparison between testing performance and manual processing with the same test dataset is provided.", "description": "We encourage authors to provide comparisons between model testing performance and manual processing performance in testing dataset. It can be used to prove the normalization of testing datasets, by checking the manual performance and existed reports.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}, {"no": 28, "label": "Details of test datasets and speculation results are provided.", "description": "We suggest authors to provide testing dataset details and result details in articles, involving the false cases and possible reasons of mistakes.", "totalScore": 2.5, "options": [{"answer": "Yes", "score": 2.5}, {"answer": "No", "score": 0.0}]}]}, {"no": "V", "label": "Accessbility", "totalScore": 25.0, "items": [{"no": 29, "label": "Codes.", "description": "We require authors to provide accessibility of codes. Codes should be acquirable by websites (like github) or additional materials of reports. For some reports belonging to ongoing research, it is understandable but don't score that codes only can be acquired by connecting with corresponding authors. Considering possible restrictions in code opening, it is acceptable if the technological details are described in articles. The score of this item is connected to the accessibility of codes. There are four situation of code accessibility: 1) Fully accessible. Codes are open-access in webistes or additional materials. 2) Partly accessible. Codes are not open. Authors describe detailed strucutres and modifications of models in articles. 3) Possible accessible. Codes are not open, and authors don't describe detailed information about applied models. 4) Not accessible. Authors don't mention about models and relative information in paper.", "totalScore": 10.0, "options": [{"answer": "Fully accessible", "score": 10.0}, {"answer": "Partly accessible", "score": 7.5}, {"answer": "Possible accessible", "score": 5.0}, {"answer": "Not accessible", "score": 0.0}]}, {"no": 30, "label": "Datasets.", "description": "We require authors to provide some information and accessibility about datasets in reports. It should be fully accessible if it is all originated from a open source(like TCIA). If the raw data includes some cases from closed sources, the variety of accessibility is connected to the scores. There are four situation of dataset accessibility: 1)Fully accessible. Raw data and after-processing data are open-access. 2) Partly accessible. Cases from open source are described, involving specific application lists and inclusion/exclusion criteria. Data from closed source are not open. Authors provide detailed description of closed data, involving general condition of cohorts, institutional ethical reviewing and approval number. Also, after-processing examples are required. 3)Possible accessible. Comparing with Partly accessible, this situation are lack of after-processing examples and detailed information of open-source application. 4) Not accessible. Authors don't provide enough information to access datasets.", "totalScore": 10.0, "options": [{"answer": "Fully accessible", "score": 10.0}, {"answer": "Partly accessible", "score": 7.5}, {"answer": "Possible accessible", "score": 5.0}, {"answer": "Not accessible", "score": 0.0}]}, {"no": 31, "label": "Weights.", "description": "The accessibility of weights is one of the most factor in reproduction. It is a main point in reproducibility assessments.", "totalScore": 5.0, "options": [{"answer": "Accessible", "score": 5.0}, {"answer": "Inaccessible", "score": 0.0}]}]}]}